# 单层感知器的结构
考虑一个二分类问题：输入是一个N维向量$x=[x_1,x_2,...,x_n]$，其中的每一个分量都对应于一个权值$\omega_i$，隐含层的输出叠加为一个标量值：$$v=\sum_{i=1}^N x_i\omega_i $$

随后在二值阈值元件中对得到的v值进行判断，产生二值输出：$$y=\begin{cases} 1, & v \ge 0 \\-1, & v \le 0 \end{cases} $$

单层感知器可以将输入数据分为了两类：$l_1$或$l_2$。当y=1时，认为输入x属于$l_1$类，y=-1时，属于$l_2$类。

在实际应用中，除了输入的N维向量外，还有一个外部偏置，值恒为1，权值为b，结构图如下所示：

![[../../Attachement/image-20221218084530705.png]]


这样，输出y可以表示为 $$y=sgn(\sum_{i=1}^N \omega_ix_i + b)$$

单层感知器进行模式识别的超平面由下式决定：

$$\sum_{i=1}^Nw_ix_i+b=0$$

当维数N=2时，输入向量可表示为平面直角坐标系中的一个点。此时分类超平面是一条直线：
$$\omega_1x_1+\omega_2x_2 + b = 0$$

# 单层感知器的学习算法

方便起见，修改单层感知器的结构图如下所示，将偏置作为一个固定输入：

![[../../Attachement/2243.jpg]]

因此，定义输入向量：
$$x(n)=[1,x_1(n),x_2(n),...,x_N(n)]^T$$

这里的n表示迭代次数。

相应的，定义权值向量：
$$w(n)=[b(n),\omega_1(n),\omega_2(n),...,\omega_N(n)]^T$$

因此，线性组合器的输出为：
$$v(n)=\sum_{i=0}^N \omega_ix_i={\omega}^T(n)x(n)$$

令上式等于零，即得二分类问题的决策面。

## 学习算法步骤
1. 定义变量和参数
    - N+1维输入向量：$x(n)=[1,x_1(n),x_2(n),...,x_N(n)]^T$
    - N+1维权值向量$w(n)=[b(n),\omega_1(n),\omega_2(n),...,\omega_N(n)]^T$
    - 偏置：$b(n)$
    - 实际输出：$y(n)$
    - 期望输出：$d(n)$
    - 学习率参数：$\eta$，是一个小于1的正常数
2. 初始化。n=0，将权值向量$\omega$设置为全零值或随机值。
3. 激活。输入训练样本，对每个训练样本$x(n)=[1,x_1(n),x_2(n),...,x_N(n)]^T$，指定其期望输出d，即若$x \in l_1，d=1$，若$x \in l2，d=-1$
4. 计算实际输出：$$y(n)=sgn\Big(v(n)\Big)=sgn\Big({\omega}^T(n)x(n)\Big)$$
5. 更新权值向量
$$\omega_{n+1}=\omega_n + \eta [d(n)- y(n)]x(n)$$
这里
$$d(n)=\begin{cases} 1, & x(n)\in l_1 \\ -1, & x \in l_2 \end{cases}$$

6. 判断是否收敛。若满足收敛条件，则算法结束，若不满足，则$n=n+1$，转到第3步继续执行。
当权值向量$\omega$已经能够正确实现分类时，算法就收敛了，此时网络的误差为零。在计算时，收敛条件通常可以是：
- 误差小于某个预先设定的较小值$\epsilon$，即$$|d(n)-y(n)| \le \epsilon$$
- 两次迭代之间的权值变化已经很小，即$$|\omega_(n+1)-\omega(n)| \le \epsilon$$
- 设定最大迭代次数M，当迭代了M次以后算法停止迭代。

# 感知器的局限性
1. 感知器的激活函数使用阈值函数，使得输出只能取两个值，这样就限制了在分类种类上的扩展
2. 感知器网络只对线性可分的问题收敛
3. 如果输入样本存在奇异样本，则网络需要花费很长的时间。奇异样本就是数值上远远偏离其他样本的数据
4. 感知器的学习算法只对单层有效，因此无法直接套用其规则设计多层感知器