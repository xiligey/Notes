线性神经网络与感知器的主要区别在于，感知器的传输函数只能输出两种可能的值，而线性神经网络的输出可以取任意值，其传输函数是线性函数。线性函数采用Least Mean Square算法来调整网络的权值和偏置。

线性神经网络在收敛的精度和速度上较感知器有了较大的提高，但其线性运算规则决定了，它只能解决线性可分的问题。

# 线性神经网络的结构

![[../../Attachement/Pasted image 20230106151441.png|700]]

如图所示，线性神经网络除了可以产生二值输出外，还可以产生模拟输出——即采用线性传输函数，使输出可以为任意值。

假设输入向量是一个N维向量$\pmb x=[x_1,x_2,...,x_N]$，从输入到神经元的权值为$\omega_i$，则改神经元的输出为：
$$v=\sum_{i=1}^Nx_i \omega_i+b$$

在输出节点中的传递函数采用线性函数purelin，其输入与输出之间是一个简单的比例关系，线性网络的最终输出为：
$$y=purelin(v)=purelin\Big(\sum_{i=1}^Nx_i \omega_i+b\Big)$$

写成矩阵的形式，假设输入向量为：
$$\pmb x(n)=[1,x_1(n),x_2(n),...,x_N(n)]^T$$

权值向量：
$$\pmb w(n)=[\omega_0(n),\omega_1(n),\omega_2(n),...,\omega_N(n)]^T$$

其中$\omega_0(n)=b(n)$，表示偏置。

则输出可以表示为：
$$\pmb y=\pmb x^T \pmb \omega$$

$$\pmb q=sgn(\pmb y)$$

若网络中包含多个神经元节点，就能形成多个输出，这种线性神经网络交Madaline网络。该网络结构图如下：

![](../../Attachement/20221220183042.png)

# LMS学习算法
线性神经网络对的闪光之处就在于其学习算法。Widrow和Hoff于1960年提出自适应滤波LMS算法，也称为$\Delta$规则(Delta Rule)。
LMS算法与感知器网络的学习算法在权值调整上都基于纠错学习规则，但LMS更易实现。

LMS算法只能训练单层网络。

定义某次迭代时的误差信号为：
$$e(n)=d(n)-\pmb x^T(n)\pmb w(n) \tag{1}$$
其中n代表迭代次数，$\pmb d$表示期望输出。这里采用均方误差作为评价指标：
$$mse=\frac 1 Q \sum_{k=1}^Q e^2(k) \tag{2}$$

Q是输入训练样本的个数。线性神经网络的学习目标就是找到适当的$\pmb \omega$使得均方误差mse最小。只要用mse对$\pmb \omega$求导，再令偏导数等于零，即可求出mse的极值。显然，mse必为正值，因此二次函数是凹向上的，求得的极值必为极小值。

在实际运算中，为了解决权值$\pmb \omega$维度过高，给计算带来困难的问题，往往是通过调节权值，使mse从空间中的某一点开始，沿着斜面向下滑行，最终达到最小值。滑行的方向是该点最陡下降的方向，即负梯度方向。沿着此方向以适当强度对权值进行修正，就能最终到达最佳权值。

实际计算中，代价函数常定义为：
$$E(\pmb \omega)=\frac 1 2 e^2(n) \tag{3}$$

对该式两边关于权值向量$\omega$求偏导，可得：
$$\frac {\partial E}{\partial \pmb \omega}=e(n)\frac {\partial e(n)}{\partial \pmb  \omega} \tag{4}$$

根据公式(1)可得：
$$\frac {\partial e(n)}{\partial \omega}=-\pmb x^T(n) \tag{5}$$

再将公式(5)带入公式(4)可得：
$$\frac {\partial E}{\partial \pmb \omega} = -\pmb x^T(n)e(n) \tag{6}$$

因此，根据梯度下降法，权矢量的修正值正比于当前位置上$E(\pmb \omega)$的梯度，权值调整的规则为：
$$\pmb \omega(n+1)=\pmb \omega(n) + \eta(- \nabla) \tag{7}$$

即

$$\pmb \omega(n+1) = \pmb \omega(n) + \eta(- \frac {\partial E}{\partial \pmb \omega})= \pmb \omega(n)+\eta \pmb x^T(n)e(n) \tag{8}$$

其中$\eta$为学习率，$\nabla$为梯度。

## LMS算法的步骤
1. 定义变量和参数
为方便处理，将偏置和权值合并
$$\pmb \omega(n)=[b(n),\omega_1(n),\omega_2(n),...,\omega_N(n)]^T$$

相应的，训练样本为
$$\pmb x(n)=[1,x_1(n),x_2(n),....,x_N(n)]^T$$

b(n)为偏置，d(n)为期望输出，$\eta$为学习率，n为迭代次数

2. 初始化。给向量$\pmb \omega(n)$赋一个较小的随机初值，n=0
3. 输入样本，计算实际输出与误差：$e(n)=d(n)-\pmb x^T(n)\pmb w(n) $
4. 调整权值向量。根据上一步算得的误差，更新权值向量：$\pmb \omega(n+1) = \pmb \omega(n)+\eta \pmb x^T(n)e(n)$
5. 判断算法是否收敛。若满足收敛，则停止迭代，不满足则返回第3步。

收敛条件：
- 误差等于0或小于某个约定值
- 权值变化很小
- 达到最大迭代次数

# LMS算法中学习率的选择
学习率越小，算法的运行时间就越长。

1996年Hyajin证明，只要学习率$\eta$满足下式，LMS算法就是按方差收敛的

$$0 < \eta < \frac 2 {\lambda_{max}}$$
其中$\lambda_{max}$是输入向量$\pmb x(n)$组成的自相关矩阵R的最大特征值。由于$\lambda_{max}$常常不可知，因此往往使用自相关矩阵R的迹来代替。按定义，矩阵的迹是矩阵主对角线元素之和：
$$tr(R)=\sum_{i=1}^Q R(i,i)$$

同时，矩阵的迹又等于矩阵所有特征值之和，因此一般有$tr(R) > \lambda_{max}$，只要取$$0<\eta<\frac 2 {tr(R) } < \frac 2 {\lambda_{max}}$$
即可满足条件。按定义，自相关矩阵的主对角线元素就是各输入向量的均方值，因此公式又可以写为：$$0 < \eta < \frac 2 {向量均方值之和}$$

## 学习率逐渐下降
学习率随着学习的进行逐渐下降比始终不变更加合理。
在学习的初期，用较大的学习率保证收敛速度，随着迭代次数的增加，减小学习率以保证精度。

方案1:$$\eta=\frac {\eta_0}{n}$$
方案2:$$\eta=c^n \eta_0$$
c是一个接近于1而小于1的常数。
方案3:$$\eta=\frac {\eta_0}{1+\frac n {\tau}} $$

$\eta_0$和$\tau$均为常量。

# LMS算法的缺点
LMS算法的一个缺点是，它对输入向量自相关矩阵R的条件数敏感。当一个矩阵的条件数比较大时，矩阵就称为病态矩阵。这种矩阵的元素做微小改变，可能会引起相应线性方程的解的很大变化。

# 线性神经网络与感知器的对比
感知器与线性神经网络在结构上非常相似，唯一的区别在于传输函数：感知器采用二值阈值函数，线性神经网络采用线性函数。

这就决定了感知器只能做二分类，而线性神经网络可以实现拟合或逼近。


LMS算法似乎与感知器学习算法没什么两样，但LMS算法得到的分类边界往往处于两类模式的正中间，而感知器学习算法在刚刚能正确分类的位置就停下来了，从而使得分类边界离一些模式距离过近，使系统误差更敏感。这一区别与两种神经网络的不同传输函数有关。
