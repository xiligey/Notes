```toc
```

## 回归问题

### MSE-均方误差
Mean Squared Error
$$L(y,\hat y)=\frac 1 n \sum_{i=1}^n(y_i-\hat y_i)^2$$
### MAE-平均绝对值误差
Mean Absolutely Error
$$L(y, \hat y)=\frac 1 n \sum_{i=1}^n|y_i-\hat {y_i}|$$


### MSE(L1) VS MAE(L2)
- 鲁棒性：MSE计算简便，但MAE对异常点有更好的鲁棒性。当数据中存在异常点时，用MSE/RMSE计算损失的模型会以牺牲了其他样本的误差为代价，朝着减小异常点误差的方向更新。然而，这会降低模型的整体性能。
> 直观上可以这样理解：如果我们最小化MSE来对所有的样本点只给出一个预测值，那么这个值一定是所有目标值的平均值。但如果是最小化MAE，那么这个值，则会是所有样本点目标值的中位数。众所周知，对异常值而言，中位数比均值更加鲁棒，因此MAE对于异常值也比MSE更稳定。
- 梯度：神经网络中，MAE更新梯度始终相同，而MSE则不同，MSE损失的梯度随着损失增大而增大，而损失趋于零时则会减小
- Loss选择建议：
	- MSE：如果异常点代表在商业中很重要的异常情况，并且需要被检测出来
	- MAE：如果只把异常值当成受损数据

### Huber损失


$$L_{\delta}(y_i- \hat {y_i})= \begin{cases} \frac 1 2 (y_i- \hat {y_i})^2 & for \ |y-f(x)| \le \delta \\ \delta|y_i- \hat {y_i}|-\frac 1 2 \delta^2 & otherwise \end{cases}$$
- 当Huber损失在$[-\delta, \delta]$之间时，等价于MSE
- 在$[-\infty, -\delta]$和$[\delta, +\infty]$时，等价于MAE

> 使用MAE训练神经网络最大的一个问题就是不变的大梯度，这可能导致在使用梯度下降快要结束时，错过了最小点。而对于MSE，梯度会随着损失的减小而减小，使结果更加精确。在这种情况下，Huber损失就非常有用。它会由于梯度的减小而落在最小值附近。比起MSE，它对异常点更加鲁棒。因此，Huber损失结合了MSE和MAE 的优点。但是，Huber损失的问题是我们可能需要不断调整超参数$\delta$

下图是Huber随着$\delta$的变化曲线。当$\delta$很大时，等价于MSE曲线，当$\delta$很小时，等价于MAE曲线


![123|500](../../Attachement/640.gif)

## 分类问题

### LogLoss-交叉熵
- [ ] [损失函数｜交叉熵损失函数 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/35709485) (@2023-01-09 10:00:00)



二分类任务中常用的损失函数。在[[广义线性模型/逻辑回归]]中，通过对似然函数取对数得到。